{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "April 2022\n",
    "\n",
    "## Advanced Legal Analytics - Tutorial 1 on Introduction to Text Processing in Python\n",
    "\n",
    "### Intended Learning Outcomes\n",
    "\n",
    "This tutorial will provide you the background knowledge to process unstructured data- text with the help of computers. You will learn different natural language processing technologies for different types of text processing using the Python programming language.  \n",
    "\n",
    "**After this tutorial you will be able to**\n",
    "- **write regular expressions to retrieve relevant text/patterns from a (legal) document**\n",
    "- **apply different steps of the natural language processing pipeline for cleaning and pre-processing the text**\n",
    "\n",
    "\n",
    "### Readings/References for the Tutorial\n",
    "\n",
    "- http://codingforlawyers.com/chapters/ch1/\n",
    "- Documentation for regular expressions: https://pyneng.readthedocs.io/en/latest/book/14_regex/index.html\n",
    "\n",
    "### Prerequisites for the Tutorial\n",
    "\n",
    "It is recommended to get an understanding of the technologies discussed in the following resources before attempting the tutorial exercises\n",
    "\n",
    "- https://app.datacamp.com/learn/courses/introduction-to-natural-language-processing-in-python\n",
    "- https://www.w3schools.com/python/python_for_loops.asp\n",
    "- https://www.datacamp.com/community/tutorials/python-list-comprehension \n",
    "\n",
    "\n",
    "### Python Libraries to be used and Installation Instructions\n",
    "\n",
    "- NLTK (https://www.nltk.org/) and pdfplumber (https://pypi.org/project/pdfplumber/)\n",
    "- For Windows: Type “Anaconda prompt” in your window search option. Right click the Anaconda Prompt and run it as administrator. Then follow the instructions here: https://www.nltk.org/install.html and https://pypi.org/project/pdfplumber/\n",
    "- For MaC OS: Type Command + Space Bar on your Mac Keyboard. Type in “Terminal”. Open the terminal. Then follow the instructions here: https://www.nltk.org/install.html and https://pypi.org/project/pdfplumber/\n",
    "\n",
    "- Also install NLTK data using the instructions here: https://www.nltk.org/data.html. You need to run the commands (`import nltk` and `nltk.download()` in a python terminal. Once the window appears you should select `all` and click Download. \n",
    "\n",
    "<hr /> <hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions (RE) or Regex\n",
    "\n",
    "A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern. Regular expressions are used to match many different types of patterns and not just exact text.\n",
    "\n",
    "##### Official Python Documentation: https://docs.python.org/3/howto/regex.html\n",
    "\n",
    "You need to import the `re` module for using regular expressions. `re` module is used for string searching and manipulation. The following functtions of regex are commonly used:\n",
    "\n",
    "- `finadall`: find all pattersn in a string and returns a list containing all matches\n",
    "- `search`: search for a pattern in a string and returns a Match object if there is a match anywhere in the string\n",
    "- `match`: match function differs from the `search` in that `match` returns the first occurrence. So, if a match is found in the first line, it returns the match object. But if a match is found in some other line, the Python RegEx Match function returns null.\n",
    "- `split`: allows you to use REs to split a string into a list\n",
    "- `sub`: Find all substrings where the RE matches, and replace them with a different string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. In the following cell, retrieve `Legal Analytics`in the given string, `text_1`. Try different functions (`search()`, `match()` and `findall()` mentioned above and comment on the similarities and differences you observe in the output of different functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text_1 = \"Legal Analytics is the prerequisite of Advanced Legal Analytics!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Split the string `text_1` into individual tokens using regular expression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Substitute 27 instead of `xx` in the sentence below using regular expressions. Store the string with 27 as replacement as `text_3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"The European Union consists of xx member states that are located primarily in Europe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metacharacters\n",
    "Read about metacharacters here: https://pynative.com/python-regex-metacharacters/\n",
    "\n",
    "##### 4. Extract digits from the modified version of text_3 using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"The European Union consists of 27 member states that are located primarily in Europe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Import the `pdfplumber` library.  Refer to the documentation of `pdfplumber` here: https://github.com/jsvine/pdfplumber and read the pdf file containing the Statute of the International Court of Justice. The file is available here: https://github.com/maastrichtlawtech/law3027-advanced-legal-analytics/blob/main/data/icj.pdf Complete the code below to extract the content of the 5th page of the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "with pdfplumber.open('icj.pdf') as pdf:\n",
    "    for page_ in pdf.pages:\n",
    "        fifth_page = # Complete here #\n",
    "print(fifth_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.  Store the entire content of the icj.pdf file in one string variable. Print this string to ensure that all the articles are contained in the string variable.\n",
    "\n",
    "Hint: Store the content of each page in a list. Use the `append()` function of the list inside a `for loop` to store each page in the list. Then use the `join()` function to convert the list to a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "list_pages_icj = []\n",
    "with pdfplumber.open('icj.pdf') as pdf:\n",
    "    for page_ in pdf.pages:\n",
    "        # write code to append page to list_pages_icj\n",
    "\n",
    "str_pages_icj = ' '.join(list_pages_icj) #This string will contain the entire content of the icj.pdf file\n",
    "str_pages_icj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.1 Try to extract all the article numbers (Article1, Article 2, Article 3........till the last article)  from the entire text of the icj.pdf document using regular expressions. Are you able to extract all the article  numbers? If so, great. If its not possible to extract some article  numbers what's the reason behind  that ? Store the extracted article numbers in a list named, `list_articles_icj`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2 Print the number of articles you were able to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.1 Now extract the text corresponding to each article number using regular expressions. Store the text corresponding to each article number in a list named, `list_text_icj`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.2 Now print the number of texts you extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9 Check if the number of extracted articles is equal to the number of extracted texts. What's the difference and how would you fix it ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Store all the extracted article numebrs and their corresponding texts in a pandas DataFrame named `df_articles_icj`. This dataframe should have 2 columns: `Article_Number` and `Article_Text`.  You must create this dataframe as follows using the two lists: `list_articles_icj` and `list_text_icj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_articles_icj = pd.DataFrame({'Article Number': list_articles_icj, \n",
    "                            'Article Text': list_text_icj   \n",
    "                           })\n",
    "df_articles_icj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can strip the newline symbol from the article number using the `replace()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_icj['Article Number'] = df_articles_icj['Article Number'].replace('\\n', '', regex =True)\n",
    "df_articles_icj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. Write the python code to extract some major keywords/entities from the entire text of `icj.pdf` file.\n",
    "Hint: You may think of major entities as names which have capital letters and atleast 2 words or 3 words. Write a regular expression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of the Tutorial:  Natural Language Pre-Processing Pipeline\n",
    "\n",
    "Before you can do any kind of machine learning on the text, it has to be properly pre-processed. This involves cleaning  and normalizing the data to reduce noise. Text preprocessing breaks down a corpus into smaller parts then extracts the most important information from those parts which a machine learning model will then derive meaning from. \n",
    "\n",
    "You cannot directly feed the text to machine learning models. So before feeding it to a model, we need to normalize it to numerical features. This process is called vectorization. But before vectorizing the text, we need to reduce the noise so that we can focus on the most important words (as they lead to key features for the machine learning model).\n",
    "\n",
    "In this tutorial, we will focus on the most important pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deleting unnecessary numerals. \n",
    "\n",
    "##### 12. Extract Article 26 from the DataFrame `df_articles_icj`. Article 26 has 3 sub-articles: 1,2 and 3. Remove the numbers 1, 2 and 3 from the text of the article (hint: use regular expressions). Print the modified Article 26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into tokens. Tokens can be words, sentences, phrases etc. depending on the text and the level of tokenization you want to apply.  The level of tokenization may depend on the separator. \n",
    "\n",
    "We will use the NLTK library for tokenization. See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text_1 = \"Legal Analytics is the prerequisite of Advanced Legal Analytics!\"\n",
    "tokens_text_1 = word_tokenize(text_1)\n",
    "tokens_text_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. Tokenize the Article 26 into words. Print the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. We can convert tokens to also lowercase to standardize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a26_lowercase = [token.lower() for token in tokens_a26]\n",
    "tokens_a26_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Word counts with bag-of-words model. Count the frequency of each word in Article 26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(tokens_a26_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Remove punctutation from Article 26\n",
    "\n",
    "Punctuation might not bring additional value for vectorization of text. Punctuation removal is generally done after the tokenization process. Removal of punctuation prior to the tokenization process might lead to incorrect tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "tokens_a26_without_punct = [t for t in tokens_a26_lowercase if t not in string.punctuation]\n",
    "tokens_a26_without_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords\n",
    "\n",
    "Stopwords are the most commonly used words in a language. The basic idea is that by removing such words we can focus on the important words instead. Note that removing stopwords will may not always be useful. It depends on the particular task you are trying to do.\n",
    "\n",
    "NLTK has a default list of stopwords which we can use. Researchers also tend to create their own list of stopwords depending on the domain (of the text corpus) and languages. We will start with NLTK's default list of stopwords.\n",
    "\n",
    "##### 17. Remove the stopwords from Article 26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "tokens_a26_without_stop_words = [t for t in tokens_a26_without_punct if t not in nltk_stop_words]\n",
    "tokens_a26_without_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming & Lemmatization\n",
    "For grammatical reasons, documents are going to use different forms of a word, such as **organize, organizes, and organizing**. Additionally, there are families of derivationally related words with similar meanings, such as **democracy, democratic, and democratization**. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "**Stemming** is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required.\n",
    "\n",
    "**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token **saw**, stemming might return just **s**, whereas lemmatization would attempt to return either **see** or **saw** depending on whether the use of the token was as a verb or a noun. In case of lemmatization, the root word, also called lemma belongs to the language. \n",
    "\n",
    "\n",
    "[Source (Refer to this tutorial for more details): https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NLTK's porter stemmer as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stem_tokens_a26 = [ps.stem(token_) for token_ in tokens_a26_without_stop_words]\n",
    "stem_tokens_a26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Lemmatization. We will use the WordNet Lemmatizer provided by NLTK. WordNet® is a large lexical database of English. You can read more about wordnet here: https://wordnet.princeton.edu/ . The wordNet lemmatizer strip affixes from word if the resulting word is in its database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens_a26 = [lemmatizer.lemmatize(token_) for token_ in tokens_a26_without_stop_words]\n",
    "lemmatized_tokens_a26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. Compare the output of the Stemming and Lemmatization output. Which one will you use and why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Part-of-speech (POS) Tagging\n",
    "Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context.\n",
    "\n",
    "Go here and try this website https://parts-of-speech.info/. Observe the POS tags for each token.\n",
    "\n",
    "We will use the NLTK library's `pos_tag()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokens_ = nltk.pos_tag(tokens_a26_without_stop_words, tagset='universal')\n",
    "pos_tokens_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
